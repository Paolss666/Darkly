# Web Directory Enumeration & Flag Extraction

## Overview

This challenge involves enumerating a web server's `.hidden/` directory structure containing multiple nested obfuscated directories with README files scattered throughout. The goal is to find and extract the flag from these files.

---

## Enumeration

The `.hidden/` directory contains:
- Multiple nested subdirectories with obfuscated names
- Random plaintext README files distributed across the directory tree
- A `robots.txt` file that restricts crawler access to the `.hidden` endpoint

---

## Solution

### Step 1: Recursive Download with wget

Use `wget` with the `--execute robots=off` option to bypass the `robots.txt` restrictions and download all files recursively:
```bash
wget --recursive --no-parent --no-check-certificate --execute robots=off http://target-url/.hidden/
```

### Step 2: Create Flag Finder Script

Create a script named `finder.sh`:
```bash
#!/bin/bash
# finder.sh

# Set the starting directory
START_DIR="${1:-.}"  # Default to current directory if not provided

# Traverse recursively and find README files (case-insensitive)
find "$START_DIR" -type f -iname "README*" | while read -r file; do
    cat "$file" | grep flag
done
```

Make it executable:
```bash
chmod +x finder.sh
```

### Step 3: Execute the Script

Run the script in the extracted directory:
```bash
./finder.sh /path/to/downloaded/.hidden
```

---

## Security Issues

1. **robots.txt bypass**: The `robots.txt` file can be specifically ignored and provides no real protection
2. **Hidden directory exposure**: Hidden directories can still be accessed manually through direct navigation
3. **Sensitive data exposure**: Sensitive information should never be made publicly servable in any directory structure

---

## Prevention

- Do not rely on `robots.txt` for security
- Implement proper access controls at the server level
- Never store sensitive information in web-accessible directories
- Use authentication and authorization mechanisms for protected content
